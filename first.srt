 Pie Torch. In open-source deep learning framework used to build some of the world's most famous artificial intelligence products. It was created at the Meta AI Research Lab in 2016, but is actually derived from the Lua-based torch library that dates back to 2002. Fundamentally, it's a library for programming with tensors, which are basically just multi-dimensional arrays that represent data and parameters in deep neural networks. Sounds complicated, but its focus on usability will have you training machine learning models with just a few lines of Python. In addition, it facilitates high-performance parallel computing on a GPU, thanks to Nvidia's CUDA platform. Developers love prototyping with it because it supports a dynamic computational graph, allowing models to be optimized at runtime. It does this by constructing a directed A-cyclic graph consisting of functions that keeps track of all the executed operations on the tensors, allowing you to change the shape, size, and operations after every iteration if needed. Pie Torch has been used to train models for computer vision AI like Tesla Autopilot, image generators like stable diffusion, and speech recognition models like OpenAI Whisper, just to name a few. To get started, install Pie Torch and optionally CUDA if you want to accelerate computing on your GPU. Now, import it into a Python file or notebook, like I mentioned, a tensor is similar to a multi-dimensional array, create a 2D array or matrix with Python, then use Torch to convert it into a tensor. Now, we can run all kinds of computations on it, like we might convert all these integers into random floating points. We can also perform linear algebra by taking multiple tensors and multiplying them together. What you came here to do, though, is build a deep neural network, like an image classifier. To handle that, we can define a new class that inherits from the neural network module class. Inside the constructor, we can build it out layer by layer. The flattened layer will take a multi-dimensional input, like an image, and convert it to one dimension. From there, sequential is used to create a container of layers that the data will flow through. Each layer has multiple nodes, where each node is like its own mini-statistical model, as each data point flows through it, it'll try to guess the output, and gradually update a mapping of weights to determine the importance of a given variable. Linear is a fully connected layer that takes the flattened 28x28 image and transforms it to an output of 512. This layer is followed by a non-linear activation function. When activated, it means that feature might be important, and outputs the node, otherwise it just outputs zero. And finally, we finish with a fully connected layer that outputs the 10 labels the model is trying to predict. With these pieces in place, that next step is to define a forward method that describes the flow of data, and now instantiate the model to a GPU, and pass its input data. This will automatically call its forward method for training and prediction. Congratulations, you just built a neural network. This has been PyTorch in 100 seconds. Thanks for watching, and I will see you in the next one.